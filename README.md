# Neural Network from Scratch 🧠

This project is a basic implementation of a feedforward neural network built entirely from scratch using Python and NumPy — without any machine learning frameworks like TensorFlow, Keras, or PyTorch.

## 🚀 Features

- Simple feedforward neural network
- Forward and backward propagation
- Sigmoid activation function
- Log loss 
- Written for learning and demonstration purposes

## 🧠 How It Works

1. **Initialize Weights** – Randomly initialize weights and biases.
2. **Forward Pass** – Calculate outputs using activation functions.
3. **Loss Calculation** – Compute error using Log Loss.
4. **Backward Pass** – Adjust weights using the derivative of the loss.
5. **Repeat** – Train over multiple epochs.

## 📊 Example Output

Trained on the dataset:
<img width="1047" height="61" alt="image" src="https://github.com/user-attachments/assets/670dd231-e528-4c86-b69e-ee49c0a507c5" />

## 📁 Project Structure
    ```plaintext
    Neural-Network/
        ├── insurance_data.csv     # Insurance small dataset
        ├── NN_from_Scratch.ipynb  # Jupyter Notebook
        ├── nn.jpg                 # NN explanation img
        ├── logloss.png            # Log loss formula img
        └── README.md              # Project documentation

## 📦 Requirements

- Python
- NumPy
- Pandas
- sklearn

## ▶️ How to run
    ```bash
    python NN_from_Scratch.ipynb


## 📚 Learning Goals
This project helped me understand:

How neural networks learn via backpropagation

How to implement gradient descent manually

Core building blocks of deep learning

🌟 Inspiration
I built this project to solidify my understanding of neural networks at the mathematical and code level, and to learn how modern deep learning models are built from the ground up.

🔗 Connect With Me
📧 Dhruv Bavaliya
📬 Feel free to contribute or fork this project if you're learning like me!


