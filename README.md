# Neural Network from Scratch ğŸ§ 

This project is a basic implementation of a feedforward neural network built entirely from scratch using Python and NumPy â€” without any machine learning frameworks like TensorFlow, Keras, or PyTorch.

## ğŸš€ Features

- Simple feedforward neural network
- Forward and backward propagation
- Sigmoid activation function
- Log loss 
- Written for learning and demonstration purposes

## ğŸ§  How It Works

1. **Initialize Weights** â€“ Randomly initialize weights and biases.
2. **Forward Pass** â€“ Calculate outputs using activation functions.
3. **Loss Calculation** â€“ Compute error using Log Loss.
4. **Backward Pass** â€“ Adjust weights using the derivative of the loss.
5. **Repeat** â€“ Train over multiple epochs.

## ğŸ“Š Example Output

Trained on the dataset:
<img width="1047" height="61" alt="image" src="https://github.com/user-attachments/assets/670dd231-e528-4c86-b69e-ee49c0a507c5" />

## ğŸ“ Project Structure
    ```plaintext
    Neural-Network/
        â”œâ”€â”€ insurance_data.csv     # Insurance small dataset
        â”œâ”€â”€ NN_from_Scratch.ipynb  # Jupyter Notebook
        â”œâ”€â”€ nn.jpg                 # NN explanation img
        â”œâ”€â”€ logloss.png            # Log loss formula img
        â””â”€â”€ README.md              # Project documentation

## ğŸ“¦ Requirements

- Python
- NumPy
- Pandas
- sklearn

## â–¶ï¸ How to run
    ```bash
    python NN_from_Scratch.ipynb


## ğŸ“š Learning Goals
This project helped me understand:

How neural networks learn via backpropagation

How to implement gradient descent manually

Core building blocks of deep learning

ğŸŒŸ Inspiration
I built this project to solidify my understanding of neural networks at the mathematical and code level, and to learn how modern deep learning models are built from the ground up.

ğŸ”— Connect With Me
ğŸ“§ Dhruv Bavaliya
ğŸ“¬ Feel free to contribute or fork this project if you're learning like me!


